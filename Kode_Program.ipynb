{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "SN98s_7TKlsA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr-Po4ruKf4T"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping Data\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "ZPvqWjNxKpEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing Data\n",
        "# Case Folding & Cleaning Data\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Sentence Splitting\n",
        "import stanza\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Normalisasi\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from indo_normalizer import Normalizer"
      ],
      "metadata": {
        "id": "zI78bU7pNQH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekstraksi Aspek\n",
        "import stanza\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LDdz2wzESdWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Klasifikasi Sentimen\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "KpHj2gCpSViB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metode Balas\n",
        "import heapq\n",
        "import math"
      ],
      "metadata": {
        "id": "iSz2oaNuVwFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Data"
      ],
      "metadata": {
        "id": "8kgMC7PBMCzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emina"
      ],
      "metadata": {
        "id": "axih2yFIMMXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"Emina\"\n",
        "expected_product = \"Sun Protection SPF 30 PA+++\"\n",
        "expected_brand = \"Emina\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/emina/sun-protection-spf-30-pa?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews_2.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "pFzZoYa4MCKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Azarine"
      ],
      "metadata": {
        "id": "-T589iuTNUum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"AzarineCosmetic\"\n",
        "expected_product = \"Hydrasoothe Sunscreen Gel SPF 45+++\"\n",
        "expected_brand = \"Azarine Cosmetic\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/azarine-cosmetic/hydrashoothe-sunscreen-gel-spf45-3?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "_mS0DG14NZ0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skin Aqua Milk"
      ],
      "metadata": {
        "id": "ez_WxBXnNaX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"SkinAquaMilk\"\n",
        "expected_product = \"UV Moisture Milk\"\n",
        "expected_brand = \"Skin Aqua\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/skin-aqua/uv-moisture-milk?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "aJiTLL5jNayp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Biore"
      ],
      "metadata": {
        "id": "oBbLb4J1NbgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"Biore\"\n",
        "expected_product = \"UV Aqua Rich Watery Essence SPF 50+ PA++++\"\n",
        "expected_brand = \"Biore\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/biore/uv-aqua-rich-watery-essence-1?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "H1JQNlydNcCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NPURE"
      ],
      "metadata": {
        "id": "-qj3lV0nNcnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"NPURE\"\n",
        "expected_product = \"Cica Beat The Sun\"\n",
        "expected_brand = \"NPURE\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/npure/cica-beat-the-sun?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "NkaPI197NdTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skin Aqua Gel"
      ],
      "metadata": {
        "id": "11cqHcYzNiSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"SkinAquaGel\"\n",
        "expected_product = \"UV Moisture Gel\"\n",
        "expected_brand = \"Skin Aqua\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/skin-aqua/uv-moisture-gel-69?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "GrSe-gDhNirb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L'Oreal Paris"
      ],
      "metadata": {
        "id": "Vp0sW3fpNi_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"LOreal\"\n",
        "expected_product = \"UV Perfect Matte & Fresh Long UV SPF 50/ PA++++\"\n",
        "expected_brand = \"L'Oreal Paris\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/l-oreal-paris/uv-perfect-matte-and-fresh-long-uv-spf-50-pa?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "nSbD4qVFNjme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NIVEA"
      ],
      "metadata": {
        "id": "ui13BZcsNkBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"NIVEA\"\n",
        "expected_product = \"Sun Protect & White Oil Control Serum SPF 50+\"\n",
        "expected_brand = \"NIVEA\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/nivea/sun-protect-and-white-oil-control-serum-spf50-pa?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "-XPY83mXNkgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carasun"
      ],
      "metadata": {
        "id": "bdMYaXRvNmW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"Carasun\"\n",
        "expected_product = \"Solar Smart UV Protector SPF45 PA++++\"\n",
        "expected_brand = \"Carasun\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/carasun/carasun-solar-smart-uv-protector-spf45-pa?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "ofbxJzdoNmyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wardah"
      ],
      "metadata": {
        "id": "UROMXQpQNnMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY\n",
        "import requests, time, os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CONFIG\n",
        "product_name = \"Wardah\"\n",
        "expected_product = \"UV Shield Airy Smooth Sunscreen Serum SPF 50 PA++++\"\n",
        "expected_brand = \"Wardah\"\n",
        "\n",
        "start_page = 1\n",
        "end_page = 300\n",
        "max_reviews = 500\n",
        "\n",
        "base_url = \"https://reviews.femaledaily.com/products/moisturizer/sun-protection-1/wardah/uv-shield-airy-smooth-sunscreen-serum-spf-50-pa-1?cat=&cat_id=0&age_range=&skin_type=&skin_tone=&skin_undertone=&hair_texture=&hair_type=&order=newest&page={}\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "# SCRAPING\n",
        "for page in range(start_page, end_page + 1):\n",
        "\n",
        "    if len(all_reviews) >= max_reviews:\n",
        "        print(\"Stop: sudah terkumpul 500 review.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\n================ PAGE {page} ================\")\n",
        "\n",
        "    retry = 0\n",
        "    max_retry = 3\n",
        "    review_cards = []\n",
        "\n",
        "    while retry < max_retry:\n",
        "        print(f\"Mengambil halaman {page} (Percobaan {retry+1}/{max_retry})\")\n",
        "        url = base_url.format(page)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Status code gagal, coba lagi.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "        # VALIDASI PRODUK\n",
        "        brand_tag = soup.select_one(\"h2.product-brand\")\n",
        "        name_tag = soup.select_one(\"h1.product-name\")\n",
        "\n",
        "        if brand_tag and name_tag:\n",
        "            brand = brand_tag.get_text(strip=True)\n",
        "            prod_name = name_tag.get_text(strip=True)\n",
        "\n",
        "            if expected_brand not in brand or expected_product not in prod_name:\n",
        "                print(\"Bukan halaman produk yang sesuai — SKIP\")\n",
        "                break\n",
        "\n",
        "        # AMBIL CARD REVIEW\n",
        "        review_cards = soup.select(\"div.review-card\")\n",
        "\n",
        "        if review_cards:\n",
        "            print(f\"Ditemukan {len(review_cards)} review pada percobaan ke-{retry+1}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Tidak menemukan review. Coba ulang.\")\n",
        "            retry += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    if not review_cards:\n",
        "        print(\"Tetap kosong setelah 3x percobaan. Halaman dilewati.\")\n",
        "        continue\n",
        "\n",
        "    # EXTRACT REVIEW\n",
        "    for card in review_cards:\n",
        "        if len(all_reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        text_tag = card.select_one(\"p.text-content\")\n",
        "        review_text = text_tag.get_text(\" \").strip() if text_tag else \"N/A\"\n",
        "\n",
        "        stars = card.select(\"div.review-card-rating-wrapper i.icon-ic_big_star_full\")\n",
        "        rating = len(stars)\n",
        "\n",
        "        date_tag = card.select_one(\"p.review-date\")\n",
        "        raw_date = date_tag.get_text(strip=True)\n",
        "\n",
        "        parsed_date = None\n",
        "\n",
        "        try:\n",
        "            parsed_date = datetime.strptime(raw_date, \"%d %b %Y\")\n",
        "        except:\n",
        "            if \"days ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(days=int(raw_date.split()[0]))\n",
        "            elif \"hours ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(hours=int(raw_date.split()[0]))\n",
        "            elif \"minutes ago\" in raw_date:\n",
        "                parsed_date = datetime.now() - timedelta(minutes=int(raw_date.split()[0]))\n",
        "            elif \"yesterday\" in raw_date.lower():\n",
        "                parsed_date = datetime.now() - timedelta(days=1)\n",
        "            elif \"today\" in raw_date.lower():\n",
        "                parsed_date = datetime.now()\n",
        "\n",
        "        formatted_date = parsed_date.strftime(\"%Y-%m-%d\") if parsed_date else \"UNKNOWN\"\n",
        "\n",
        "        all_reviews.append({\n",
        "            \"Review Text\": review_text,\n",
        "            \"Rating\": rating,\n",
        "            \"Date\": formatted_date\n",
        "        })\n",
        "\n",
        "    print(f\"Total terkumpul sekarang: {len(all_reviews)}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# SAVE\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "folder = \"/content/drive/MyDrive/FD_Scraping_500\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "save_path = f\"{folder}/{product_name}_500reviews.xlsx\"\n",
        "df.to_excel(save_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nSELESAI!\")\n",
        "print(\"File tersimpan:\", save_path)\n",
        "print(\"Total review:\", len(df))\n",
        "\n",
        "# DISTRIBUSI TAHUN\n",
        "df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
        "print(\"\\nDistribusi review per tahun:\")\n",
        "print(df['Year'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "I0HYlRr2Nnq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "1sWTT4JkQJ85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gabungkan Data"
      ],
      "metadata": {
        "id": "snUmKQdlfRlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# CONFIG\n",
        "input_folder = \"/content/drive/MyDrive/Skripsi/FD_Scraping_500\"\n",
        "output_folder = \"/content/drive/MyDrive/Skripsi/Preprocessing\"\n",
        "output_file = f\"{output_folder}/AllProducts_500reviews_raw.xlsx\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Mapping huruf ID berdasarkan produk\n",
        "product_code = {\n",
        "    \"Emina\": \"A\",\n",
        "    \"AzarineCosmetic\": \"B\",\n",
        "    \"SkinAquaMilk\": \"C\",\n",
        "    \"Biore\": \"D\",\n",
        "    \"NPURE\": \"E\",\n",
        "    \"SkinAquaGel\": \"F\",\n",
        "    \"LOrealParis\": \"G\",\n",
        "    \"NIVEA\": \"H\",\n",
        "    \"Carasun\": \"I\",\n",
        "    \"Wardah\": \"J\"\n",
        "}\n",
        "\n",
        "# LOAD & COMBINE EXCEL\n",
        "excel_files = glob.glob(f\"{input_folder}/*.xlsx\")\n",
        "all_data = []\n",
        "\n",
        "print(\"File ditemukan untuk digabung:\")\n",
        "for file in excel_files:\n",
        "    print(\" -\", file)\n",
        "\n",
        "    # Ambil nama produk dari nama file\n",
        "    base_name = os.path.basename(file).replace(\"_500reviews.xlsx\", \"\")\n",
        "\n",
        "    # Baca Excel\n",
        "    df = pd.read_excel(file)\n",
        "\n",
        "    # Tambahkan kolom produk\n",
        "    df[\"Product\"] = base_name\n",
        "\n",
        "    # Tentukan kode huruf produk\n",
        "    code = product_code.get(base_name, \"X\")  # fallback X kalau tak ditemukan\n",
        "\n",
        "    # Buat ReviewID: A001, A002, dst\n",
        "    df[\"ReviewID\"] = [f\"{code}{str(i+1).zfill(3)}\" for i in range(len(df))]\n",
        "\n",
        "    all_data.append(df)\n",
        "\n",
        "# Gabungkan semua\n",
        "df_all = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "print(\"\\nTotal review gabungan:\", len(df_all))\n",
        "\n",
        "# SAVE TO DRIVE\n",
        "df_all.to_excel(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"\\nSELESAI! DATA TERSIMPAN\")\n",
        "print(\"Lokasi file:\", output_file)"
      ],
      "metadata": {
        "id": "-nJ-qVzYQNUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case Folding & Cleaning Data"
      ],
      "metadata": {
        "id": "ywMU25jOJEtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower() # Case Folding\n",
        "\n",
        "    text = re.sub(r\"http\\S+|www\\S+|bit.ly\\S+\", \"\", text)        # hapus URL\n",
        "    text = re.sub(r\"[^\\w\\s,.!?]\", \" \", text)                   # hapus emoji & symbols\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()                   # normalisasi spasi\n",
        "\n",
        "    return text\n",
        "\n",
        "df_all[\"Clean Review\"] = df_all[\"Review Text\"].apply(clean_text)\n",
        "df_all = df_all.drop_duplicates(subset=[\"Clean Review\"], keep=\"first\")\n",
        "\n",
        "output_folder = \"/content/drive/MyDrive/Skripsi/Preprocessing\"\n",
        "output_file = f\"{output_folder}/DataClean.xlsx\"\n",
        "\n",
        "df_all.to_excel(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"\\nSELESAI! DATA TERSIMPAN\")\n",
        "print(\"Lokasi file:\", output_file)"
      ],
      "metadata": {
        "id": "YqRpY-m7XR4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Splitting"
      ],
      "metadata": {
        "id": "gkz95y4bJRGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# Download Stanza Indonesian model\n",
        "stanza.download(\"id\")\n",
        "\n",
        "# Load Stanza pipeline\n",
        "nlp = stanza.Pipeline(lang=\"id\", processors=\"tokenize\")\n",
        "\n",
        "# Load clean dataset\n",
        "file_path = \"/content/drive/MyDrive/Skripsi/Preprocessing/DataClean.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Function for sentence splitting\n",
        "def stanza_sentence_split(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    doc = nlp(text)\n",
        "    return [sentence.text.strip() for sentence in doc.sentences]\n",
        "\n",
        "print(\"Memulai sentence splitting (Stanza).\")\n",
        "\n",
        "# Apply sentence splitting\n",
        "df[\"Sentences\"] = df[\"Cleaned Review\"].progress_apply(stanza_sentence_split)\n",
        "\n",
        "# Expand sentences into multiple rows\n",
        "df_sentences = df.explode(\"Sentences\").reset_index(drop=True)\n",
        "\n",
        "# Rename column\n",
        "df_sentences = df_sentences.rename(columns={\"Sentences\": \"Sentence\"})\n",
        "\n",
        "# Drop empty entries\n",
        "df_sentences = df_sentences[df_sentences[\"Sentence\"].str.strip().astype(bool)]\n",
        "\n",
        "# Export\n",
        "output_path = \"/content/drive/MyDrive/Skripsi/Preprocessing/DataSentences.xlsx\"\n",
        "df_sentences.to_excel(output_path, index=False)\n",
        "\n",
        "print(\"\\nSentence Splitting selesai.\")\n",
        "print(\"Output file:\", output_path)\n",
        "print(\"Total kalimat:\", len(df_sentences))\n",
        "print(\"Jumlah kalimat per produk:\")\n",
        "print(df_sentences[\"Product\"].value_counts())"
      ],
      "metadata": {
        "id": "v6e7J8_FHTwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalisasi"
      ],
      "metadata": {
        "id": "Y-CHT5cTKLMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from indo_normalizer import Normalizer\n",
        "\n",
        "# Load data sentence split\n",
        "input_path = \"/content/drive/MyDrive/Skripsi/Preprocessing/DataSentences.xlsx\"\n",
        "df = pd.read_excel(input_path)\n",
        "\n",
        "normalizer = Normalizer()\n",
        "tqdm.pandas(desc=\"Normalizing text\")\n",
        "\n",
        "def normalize_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    try:\n",
        "        return normalizer.normalize_text(str(text).lower())\n",
        "    except:\n",
        "        return str(text).lower()\n",
        "\n",
        "df[\"Normalized_Sentence\"] = df[\"Sentence\"].progress_apply(normalize_text)\n",
        "\n",
        "# Save output ke Drive\n",
        "output_path = \"/content/drive/MyDrive/Skripsi/Preprocessing/DataNormalized.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "print(\"Normalisasi selesai.\")\n",
        "print(\"Output file:\", output_path)"
      ],
      "metadata": {
        "id": "e3Pp_3XGHs0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ekstraksi Aspek"
      ],
      "metadata": {
        "id": "b0FeLgIPLDts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "\n",
        "stanza.download(\"id\")\n",
        "stanza.download(\"en\")\n",
        "nlp_id = stanza.Pipeline(\"id\", processors=\"tokenize,pos,lemma,depparse\")\n",
        "nlp_en = stanza.Pipeline(\"en\", processors=\"tokenize,pos,lemma,depparse\")\n",
        "\n",
        "def extract_single_word_nouns(doc):\n",
        "    aspects = []\n",
        "    for sent in doc.sentences:\n",
        "        for w in sent.words:\n",
        "            if w.upos == \"NOUN\":\n",
        "                aspects.append(w.lemma.lower())   # lemma biar seragam\n",
        "    return aspects\n",
        "\n",
        "# Load data\n",
        "input_path = \"/content/drive/MyDrive/Skripsi/Preprocessing/DataNormalized.xlsx\"\n",
        "df = pd.read_excel(input_path)\n",
        "\n",
        "aspect_rows = []\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    text = str(row[\"Normalized_Sentence\"])\n",
        "\n",
        "    lang = detect_lang_simple(text)\n",
        "    doc = nlp_id(text) if lang == \"id\" else nlp_en(text)\n",
        "\n",
        "    aspects = extract_single_word_nouns(doc)\n",
        "\n",
        "    for asp in aspects:\n",
        "        aspect_rows.append({\n",
        "            \"SentenceID\": row[\"SentenceID\"],\n",
        "            \"Product\": row[\"Product\"],\n",
        "            \"Text\": text,\n",
        "            \"Aspect\": asp\n",
        "        })\n",
        "\n",
        "df_aspect = pd.DataFrame(aspect_rows)\n",
        "\n",
        "# Filtering aspek\n",
        "# Hapus aspek yang frekuensinya kurang dari 10\n",
        "min_freq = 10\n",
        "freq = df_aspect[\"Aspect\"].value_counts()\n",
        "valid_aspects = freq[freq >= min_freq].index.tolist()\n",
        "df_filtered = df_aspect[df_aspect[\"Aspect\"].isin(valid_aspects)]\n",
        "\n",
        "# Filtering manual\n",
        "aspects_to_remove = [\n",
        "    \"sih\", \"pokok\", \"banget\", \"gitu\", \"aqua\", \"soal\", \"emina\", \"wardah\",\n",
        "    \"pas\", \"azarine\", \"npure\", \"cinta\", \"benar\", \"enggak\", \"kayak\", \"kata\",\n",
        "    \"sayang\", \"carasun\", \"loreal\", \"nih\", \"gue\", \"nivea\", \"plus\", \"bagi\", \"aktifitas\",\n",
        "    \"bawah\", \"ada\", \"biore\", \"guys\", \"segi\", \"macam\", \"nya\", \"terus\", \"sumpah\", \"enak\",\n",
        "    \"saat\", \"kalo\", \"dan\", \"so\", \"mana\", \"nama\", \"gini\", \"poll\", \"hal\", \"gara\", \"giat\",\n",
        "    \"guy\", \"ini\", \"love\", \"kemana\", \"tabur\", \"kurang\", \"huhu\", \"pol\", \"iseng\", \"thanks\",\n",
        "    \"buat\", \"sebul\", \"oke\", \"pasti\", \"dll\", \"hehe\", \"mantap\", \"suka\", \"bikin\", \"tea\",\n",
        "    \"masa\", \"far\", \"wkwk\", \"tuh\", \"menurutku\", \"lebih\", \"poin\", \"deh\", \"milik\", \"rada\",\n",
        "    \"pun\", \"tadi\", \"drama\", \"secinta\", \"female\", \"dong\", \"eh\", \"juga\", \"overall\", \"but\",\n",
        "    \"course\", \"saran\", \"apa\", \"inti\", \"agak\", \"atas\", \"bintang\", \"umumnya\", \"cukup\", \"haha\",\n",
        "    \"sunscreen\", \"sinar\", \"hasil\", \"matahari\", \"bedak\", \"skincare\", \"make\", \"teman\", \"lindung\",\n",
        "    \"jenis\", \"up\", \"moisturizer\", \"aktivitas\", \"base\", \"orang\", \"masalah\", \"outdoor\", \"review\",\n",
        "    \"ruang\", \"papar\", \"sekarang\", \"remaja\", \"area\", \"guna\", \"sun\", \"bekas\", \"serum\", \"rumah\",\n",
        "    \"sma\", \"kilang\", \"minus\", \"pelajar\", \"primer\", \"awan\", \"anak\", \"no\", \"pemula\", \"siang\",\n",
        "    \"tempat\", \"moisture\", \"menit\", \"foundation\", \"kuliah\", \"semi\", \"smp\", \"akhir\", \"hati\",\n",
        "    \"kondisi\", \"lama\", \"layer\", \"sekolah\", \"olahraga\", \"luar\", \"indoor\", \"pagi\", \"cuaca\",\n",
        "    \"seharian\", \"pengalaman\", \"kalangan\", \"ibu\", \"traveling\", \"kesan\", \"tabir\", \"surya\",\n",
        "    \"tas\", \"masker\", \"kuning\", \"mahasiswa\", \"wudhu\", \"butuh\", \"daerah\", \"preparation\",\n",
        "    \"panas\", \"auto\", \"cuci\", \"cushion\", \"pouch\", \"gampang\", \"cocok\", \"toner\", \"juara\",\n",
        "    \"cakey\", \"crack\", \"tua\", \"hype\", \"event\", \"jalan\", \"manfaat\", \"rawat\", \"beauty\", \"pantai\",\n",
        "    \"sunblock\", \"untung\", \"rating\", \"lapang\", \"pink\", \"step\", \"complexion\", \"tambah\", \"nilai\",\n",
        "    \"ekstra\", \"rangkai\", \"loose\", \"malam\", \"sekali\", \"merah\", \"sisa\", \"mama\", \"cowok\", \"skip\",\n",
        "    \"sehat\", \"terik\", \"list\", \"adek\", \"kakak\", \"lapis\", \"kena\", \"waktu\", \"mini\", \"travelling\",\n",
        "    \"white\", \"skin\", \"air\", \"daya\", \"hydrasoothe\", \"beat\", \"dna\", \"shield\"\n",
        "]\n",
        "\n",
        "df = df_filtered[~df_filtered[\"Aspect\"].isin(aspects_to_remove)]\n",
        "\n",
        "# Simpan data dengan aspek\n",
        "output_aspect = \"/content/drive/MyDrive/Skripsi/Aspect_Extraction/Data_Aspek.xlsx\"\n",
        "df.to_excel(output_aspect, index=False)\n",
        "\n",
        "# Hitung frekuensi aspek\n",
        "aspect_freq = (\n",
        "    df[\"Aspect\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"index\": \"Aspect\", \"Aspect\": \"Frequency\"})\n",
        ")\n",
        "\n",
        "aspect_freq\n",
        "\n",
        "# Simpan frekuensi aspek untuk proses pengelompokan ke fitur dan sub-fitur\n",
        "output_freq = \"/content/drive/MyDrive/Skripsi/Aspect_Extraction/Frekuensi_Aspek.xlsx\"\n",
        "aspect_freq.to_excel(output_freq, index=False)\n",
        "\n",
        "# Mapping hasil pengelompokan\n",
        "# Load file Excel frekuensi + kelompok\n",
        "df_map = pd.read_excel(\"/content/drive/MyDrive/Skripsi/Aspect_Extraction/Frekuensi_Aspek.xlsx\")\n",
        "\n",
        "# Rename 'Frequency' ke 'Aspect'\n",
        "if \"Frequency\" in df_map.columns:\n",
        "    df_map = df_map.rename(columns={\"Frequency\": \"Aspect\"})\n",
        "\n",
        "# Memastikan kolom 'Aspect' lowercase dan stripped\n",
        "df[\"Aspect\"] = df[\"Aspect\"].str.lower().str.strip()\n",
        "df_map[\"Aspect\"] = df_map[\"Aspect\"].str.lower().str.strip()\n",
        "\n",
        "# Merge berdasarkan 'Aspect'\n",
        "df_joined = df.merge(\n",
        "    df_map[[\"Aspect\", \"Jenis_Fitur\", \"Sub_Fitur\"]],\n",
        "    on=\"Aspect\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Save\n",
        "df_joined.to_excel(\"/content/drive/MyDrive/Skripsi/Aspect_Extraction/Aspek_Final.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "IJkwRq4DK9FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Klasifikasi Sentimen"
      ],
      "metadata": {
        "id": "H0hOX8nlRLKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IndoBERT"
      ],
      "metadata": {
        "id": "CxfuWfMzTr8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load data\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Skripsi/Aspect_Extraction/Aspek_Final.xlsx\")\n",
        "\n",
        "# Load model\n",
        "model_dir = \"/content/drive/MyDrive/Skripsi/Sentimen/Model_IndoBERT_ABSA_Finetuned\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "model.eval()\n",
        "\n",
        "results = []\n",
        "\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    text_input = f\"{row['Text']} [SEP] {row['Aspect']}\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text_input,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits.squeeze().numpy()\n",
        "    exp_logits = np.exp(logits)\n",
        "    softmax = exp_logits / exp_logits.sum()\n",
        "\n",
        "    results.append({\n",
        "        \"SentenceID\": row[\"SentenceID\"],\n",
        "        \"Product\": row[\"Product\"],\n",
        "        \"Text\": row[\"Text\"],\n",
        "        \"Aspect\": row[\"Aspect\"],\n",
        "        \"Jenis_Fitur\": row[\"Jenis_Fitur\"],\n",
        "        \"Sub_Fitur\": row[\"Sub_Fitur\"],\n",
        "        \"Logits_Negatif\": logits[0],\n",
        "        \"Logits_Netral\": logits[1],\n",
        "        \"Logits_Positif\": logits[2],\n",
        "        \"Prob_Negatif\": softmax[0],\n",
        "        \"Prob_Netral\": softmax[1],\n",
        "        \"Prob_Positif\": softmax[2],\n",
        "    })\n",
        "\n",
        "df_sentimen = pd.DataFrame(results)\n",
        "\n",
        "# Pelabelan sentimen\n",
        "sentiment_map = {\n",
        "    0: \"Negatif\",\n",
        "    1: \"Netral\",\n",
        "    2: \"Positif\"\n",
        "}\n",
        "\n",
        "df_sentimen[\"Sentimen\"] = (\n",
        "    df[[\"Prob_Negatif\", \"Prob_Netral\", \"Prob_Positif\"]]\n",
        "    .values\n",
        "    .argmax(axis=1)\n",
        ")\n",
        "\n",
        "df_sentimen[\"Sentimen\"] = df_sentimen[\"Sentimen\"].map(sentiment_map)\n",
        "\n",
        "# Atur ulang urutan kolom\n",
        "df_fixed = df_sentimen[[\n",
        "    \"SentenceID\",\n",
        "    \"Product\",\n",
        "    \"Text\",\n",
        "    \"Aspect\",\n",
        "    \"Jenis_Fitur\",\n",
        "    \"Sub_Fitur\",\n",
        "    \"Sentimen\",\n",
        "    \"Prob_Negatif\",\n",
        "    \"Prob_Netral\",\n",
        "    \"Prob_Positif\",\n",
        "    \"Logits_Negatif\",\n",
        "    \"Logits_Netral\",\n",
        "    \"Logits_Positif\"\n",
        "]]\n",
        "\n",
        "df_fixed.to_excel(\"/content/drive/MyDrive/Skripsi/Sentimen/Data_FINAL.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "R7FWzFc7RqQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pivot Berdasarkan Distribusi Sentimen"
      ],
      "metadata": {
        "id": "BNHQFyDTTfsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Jenis Fitur\n",
        "pivot_jenis = (\n",
        "    df_fixed\n",
        "    .groupby([\"Jenis_Fitur\", \"Sentimen\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "pivot_jenis[\"Total\"] = pivot_jenis.sum(axis=1)\n",
        "\n",
        "# Sub-Fitur\n",
        "pivot_sub = (\n",
        "    df_fixed\n",
        "    .groupby([\"Sub_Fitur\", \"Sentimen\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "pivot_sub[\"Total\"] = pivot_sub.sum(axis=1)\n",
        "\n",
        "# Produk\n",
        "pivot_produk = (\n",
        "    df_fixed\n",
        "    .groupby([\"Product\", \"Sentimen\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "pivot_produk[\"Total\"] = pivot_produk.sum(axis=1)\n",
        "\n",
        "# Produk x Jenis Fitur\n",
        "pivot_produk_jenis = (\n",
        "    df_fixed\n",
        "    .groupby([\"Product\", \"Jenis_Fitur\", \"Sentimen\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "pivot_produk_jenis[\"Total\"] = pivot_produk_jenis.sum(axis=1)\n",
        "\n",
        "# Produk x Sub-Fitur\n",
        "pivot_produk_sub = (\n",
        "    df_fixed\n",
        "    .groupby([\"Product\", \"Sub_Fitur\", \"Sentimen\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "pivot_produk_sub[\"Total\"] = pivot_produk_sub.sum(axis=1)\n",
        "\n",
        "# Jenis Fitur x Sub-Fitur\n",
        "pivot_jenis_sub = (\n",
        "    df_fixed\n",
        "    .groupby([\"Jenis_Fitur\", \"Sub_Fitur\", \"Sentimen\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "pivot_jenis_sub[\"Total\"] = pivot_jenis_sub.sum(axis=1)\n",
        "\n",
        "# Lengkap\n",
        "pivot_lengkap = (\n",
        "    df_fixed\n",
        "    .groupby([\"Product\",\"Jenis_Fitur\", \"Sub_Fitur\", \"Sentimen\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "pivot_lengkap[\"Total\"] = pivot_lengkap.sum(axis=1)\n",
        "\n",
        "# Simpan ke Excel\n",
        "with pd.ExcelWriter(\"/content/drive/MyDrive/Skripsi/Pivot_Sentimen.xlsx\") as writer:\n",
        "    pivot_jenis.to_excel(writer, sheet_name=\"Jenis_Fitur\")\n",
        "    pivot_sub.to_excel(writer, sheet_name=\"Sub_Fitur\")\n",
        "    pivot_produk.to_excel(writer, sheet_name=\"Produk\")\n",
        "    pivot_produk_jenis.to_excel(writer, sheet_name=\"ProdukxJenis\")\n",
        "    pivot_produk_sub.to_excel(writer, sheet_name=\"ProdukxSub\")\n",
        "    pivot_jenis_sub.to_excel(writer, sheet_name=\"JenisxSub\")\n",
        "    pivot_lengkap.to_excel(writer, sheet_name=\"Lengkap\")"
      ],
      "metadata": {
        "id": "T8Mt_XAaTqGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pivot Berdasarkan Rata-rata Skor Probabilitas Sentimen Positif"
      ],
      "metadata": {
        "id": "d5URKcHJTkNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Jenis Fitur\n",
        "avg_pos_jenis = (\n",
        "    df_fixed\n",
        "    .groupby(\"Jenis_Fitur\")[\"Prob_Positif\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .sort_values(\"Prob_Positif\", ascending=False)\n",
        ")\n",
        "\n",
        "# Sub-Fitur\n",
        "avg_pos_sub = (\n",
        "    df_fixed\n",
        "    .groupby(\"Sub_Fitur\")[\"Prob_Positif\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .sort_values(\"Prob_Positif\", ascending=False)\n",
        ")\n",
        "\n",
        "\n",
        "# Produk x Jenis Fitur\n",
        "avg_pos_jenis_produk = (\n",
        "    df_fixed\n",
        "    .groupby([\"Product\", \"Jenis_Fitur\"])[\"Prob_Positif\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .sort_values([\"Product\", \"Prob_Positif\"], ascending=[True, False])\n",
        ")\n",
        "\n",
        "# Produk x Sub-Fitur\n",
        "avg_pos_sub_produk = (\n",
        "    df_fixed\n",
        "    .groupby([\"Product\", \"Sub_Fitur\"])[\"Prob_Positif\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .sort_values([\"Product\", \"Prob_Positif\"], ascending=[True, False])\n",
        ")\n",
        "\n",
        "# Lengkap\n",
        "avg_lengkap = (\n",
        "    df_fixed\n",
        "    .groupby([\"Product\", \"Jenis_Fitur\", \"Sub_Fitur\"])[\"Prob_Positif\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .sort_values([\"Product\", \"Prob_Positif\"], ascending=[True, False])\n",
        ")\n",
        "\n",
        "with pd.ExcelWriter(\"/content/drive/MyDrive/Skripsi/RataRata_Sentimen_Positif.xlsx\") as writer:\n",
        "    avg_pos_jenis.to_excel(writer, sheet_name=\"Jenis_Fitur\", index=False)\n",
        "    avg_pos_sub.to_excel(writer, sheet_name=\"Sub_Fitur\", index=False)\n",
        "    avg_pos_jenis_produk.to_excel(writer, sheet_name=\"Jenis×Produk\", index=False)\n",
        "    avg_pos_sub_produk.to_excel(writer, sheet_name=\"Sub×Produk\", index=False)\n",
        "    avg_lengkap.to_excel(writer, sheet_name=\"Lengkap\", index=False)\n"
      ],
      "metadata": {
        "id": "Ym9yhx6eTj3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metode Balas"
      ],
      "metadata": {
        "id": "p9ZQO9QbTZwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "import math\n",
        "\n",
        "class BalasBestFirst:\n",
        "    def __init__(self, P, p, k, B):\n",
        "        self.P = P\n",
        "        self.p = p\n",
        "        self.k = k\n",
        "        self.B = B\n",
        "        self.n = len(P)\n",
        "        self.target_y = self.n - k\n",
        "        self.target_budget = sum(p) - B\n",
        "\n",
        "        # Urutkan variabel berdasarkan koefisien objektif (menaik) untuk minimisasi\n",
        "        self.idx_sorted_P = sorted(range(self.n), key=lambda i: P[i])\n",
        "        # self.order adalah indeks 1-based dari variabel y (sesuai P_sorted)\n",
        "        self.order = [i + 1 for i in self.idx_sorted_P]\n",
        "\n",
        "        self.best_value = math.inf # Untuk minimisasi Z'\n",
        "        self.best_solution_y = None # Solusi optimal dalam variabel y\n",
        "\n",
        "        self.node_id = -1 # Ubah ke -1 agar root node bernilai 0\n",
        "        self.counter = 0 # Untuk tie-breaker di heapq\n",
        "        self.nodes = []\n",
        "\n",
        "    # =============================\n",
        "    # CEK PELANGGARAN FIXED SECARA LANGSUNG (STRICTER PARTIAL FEASIBILITY)\n",
        "    # Ini memeriksa apakah bagian yang sudah fixed_y sudah melanggar salah satu kendala\n",
        "    # yang tidak mungkin lagi diperbaiki, bahkan dengan variabel bebas.\n",
        "    # Ini menerapkan 'Σy_i <= target_y' dan 'Σp_i*y_i >= target_budget' secara ketat pada bagian fixed.\n",
        "    # =============================\n",
        "    def check_fixed_violation(self, fixed_y):\n",
        "        current_y_ones = sum(1 for val in fixed_y.values() if val == 1)\n",
        "\n",
        "        # 1. Kendala: Σy_i <= target_y (dari Σy_i = target_y)\n",
        "        # Jika sudah terlalu banyak y=1 yang fixed, maka sudah tidak feasible.\n",
        "        if current_y_ones > self.target_y:\n",
        "            return True # Melanggar: terlalu banyak y=1, tidak bisa memenuhi Σy_i = target_y\n",
        "\n",
        "        # 2. Kendala: Σp_i*y_i >= target_budget\n",
        "        # Periksa apakah bahkan dengan semua variabel bebas diset 1, kita masih tidak mencapai target budget\n",
        "        current_py_sum = sum(self.p[i-1] * fixed_y[i] for i in fixed_y if fixed_y[i] == 1)\n",
        "        free_indices_y = [i for i in range(1, self.n + 1) if i not in fixed_y]\n",
        "\n",
        "        # Maksimum p*y yang bisa ditambahkan dari variabel bebas\n",
        "        max_possible_additional_py = sum(self.p[i-1] for i in free_indices_y)\n",
        "\n",
        "        if (current_py_sum + max_possible_additional_py) < self.target_budget:\n",
        "            return True # Melanggar: tidak mungkin memenuhi Σp_i*y_i >= target_budget\n",
        "\n",
        "        return False # Belum ada pelanggaran langsung dari bagian fixed\n",
        "\n",
        "    # =============================\n",
        "    # CEK KEMUNGKINAN PENYELESAIAN (UNTUK SEMUA KENDALA)\n",
        "    # Ini memeriksa apakah solusi parsial *masih mungkin* diperluas menjadi solusi feasible.\n",
        "    # Menerapkan 'Σy_i >= target_y' dan 'Σy_i <= target_y' serta 'Σp_i*y_i >= target_budget' pada skenario optimis.\n",
        "    # =============================\n",
        "    def is_possible_completion(self, fixed_y):\n",
        "        current_y_ones = sum(1 for val in fixed_y.values() if val == 1)\n",
        "        free_vars_count = self.n - len(fixed_y)\n",
        "\n",
        "        # --- Bagian 1: Memastikan Σy_i = target_y masih mungkin ---\n",
        "        # Ini dipecah menjadi dua pertidaksamaan:\n",
        "\n",
        "        # a) Kendala: Σy_i >= target_y\n",
        "        # Maksimum 1s yang bisa didapat (fixed 1s + semua free 1s)\n",
        "        max_possible_total_y_ones = current_y_ones + free_vars_count\n",
        "        if max_possible_total_y_ones < self.target_y:\n",
        "            return False # Tidak mungkin mencapai target_y (terlalu sedikit 1s)\n",
        "\n",
        "        # b) Kendala: Σy_i <= target_y\n",
        "        # Minimum 1s yang bisa didapat (fixed 1s + semua free 0s)\n",
        "        min_possible_total_y_ones = current_y_ones\n",
        "        if min_possible_total_y_ones > self.target_y:\n",
        "            return False # Sudah terlalu banyak 1s, tidak mungkin target_y tercapai\n",
        "\n",
        "        # --- Bagian 2: Memastikan Σp_i*y_i >= target_budget masih mungkin ---\n",
        "        fixed_py_sum = sum(self.p[i-1] * fixed_y[i] for i in fixed_y if fixed_y[i] == 1)\n",
        "        free_indices_y = [i for i in range(1, self.n+1) if i not in fixed_y]\n",
        "\n",
        "        # Untuk memaksimalkan Σp_i*y_i dari variabel bebas, kita harus memilih yang p_i terbesar.\n",
        "        free_items_p_sorted_desc = []\n",
        "        for y_idx in free_indices_y:\n",
        "            free_items_p_sorted_desc.append((self.p[y_idx-1], y_idx))\n",
        "        free_items_p_sorted_desc.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Jumlah y=1 yang harus diambil dari variabel bebas untuk mencapai target_y\n",
        "        # Ini adalah jumlah optimis untuk mencoba memenuhi kendala Σy_i = target_y\n",
        "        # dan kemudian memeriksa kendala budget.\n",
        "        ones_needed_from_free_optimistic = self.target_y - current_y_ones\n",
        "\n",
        "        # Hitung kontribusi p*y maksimum dari variabel bebas\n",
        "        # Ambil sejumlah 'ones_needed_from_free_optimistic' variabel dengan p_i terbesar.\n",
        "        max_additional_py_from_free = sum(item[0] for item in free_items_p_sorted_desc[:max(0, ones_needed_from_free_optimistic)])\n",
        "\n",
        "        if (fixed_py_sum + max_additional_py_from_free) < self.target_budget:\n",
        "            return False # Tidak mungkin memenuhi kendala budget\n",
        "\n",
        "        return True\n",
        "\n",
        "    # =============================\n",
        "    # LOWER BOUND (untuk tujuan minimisasi W')\n",
        "    # =============================\n",
        "    def lower_bound(self, fixed_y):\n",
        "        # Bound = sum(P_i * y_i) untuk variabel yang sudah fixed_y\n",
        "        z_bound = sum(self.P[i-1] * fixed_y[i] for i in fixed_y if fixed_y[i] == 1)\n",
        "\n",
        "        # Jumlah y=1 yang masih perlu ditambahkan (dari variabel bebas) untuk mencapai target_y\n",
        "        remaining_ones_to_add = self.target_y - sum(fixed_y.values())\n",
        "\n",
        "        # Tambahkan P_i dari variabel bebas yang tersisa, pilih yang P_i terkecil\n",
        "        # self.order sudah dalam urutan P_i terkecil\n",
        "        for y_idx in self.order:\n",
        "            if y_idx not in fixed_y and remaining_ones_to_add > 0:\n",
        "                z_bound += self.P[y_idx-1]\n",
        "                remaining_ones_to_add -= 1\n",
        "\n",
        "        return z_bound\n",
        "\n",
        "    # =============================\n",
        "    # ALGORITMA BALAS (BEST-FIRST)\n",
        "    # =============================\n",
        "    def solve(self):\n",
        "        pq = [] # Priority queue untuk best-first search\n",
        "        self.counter = 0 # Tie-breaker untuk heapq\n",
        "\n",
        "        # Node awal: {bound, counter, fixed_y, parent_node_id, decision_string}\n",
        "        self.counter += 1\n",
        "        heapq.heappush(pq, (0, self.counter, {}, None, \"root\"))\n",
        "\n",
        "        while pq:\n",
        "            lb, _, fixed_y, parent_node_id, decision_str = heapq.heappop(pq)\n",
        "            self.node_id += 1\n",
        "            current_node_id = self.node_id\n",
        "\n",
        "            status = \"EXPANDED\"\n",
        "            reason = \"\"\n",
        "\n",
        "            # Pruning berdasarkan incumbent solution (best_value yang ditemukan sejauh ini)\n",
        "            if lb >= self.best_value:\n",
        "                status, reason = \"PRUNED\", \"BOUND\"\n",
        "            # Pruning berdasarkan pelanggaran fixed_y secara langsung\n",
        "            elif self.check_fixed_violation(fixed_y):\n",
        "                status, reason = \"PRUNED\", \"INFEASIBLE PARTIAL (VIOLATION)\"\n",
        "            # Pruning berdasarkan kemungkinan penyelesaian (possible completion)\n",
        "            elif not self.is_possible_completion(fixed_y):\n",
        "                status, reason = \"PRUNED\", \"NOT POSSIBLE TO COMPLETE\"\n",
        "\n",
        "            self.nodes.append({\n",
        "                \"Node\": current_node_id,\n",
        "                \"Parent\": parent_node_id,\n",
        "                \"Decision\": decision_str,\n",
        "                \"Fixed_Y\": fixed_y.copy(),\n",
        "                \"LB\": round(lb, 6),\n",
        "                \"Status\": status,\n",
        "                \"Reason\": reason\n",
        "            })\n",
        "\n",
        "            if status != \"EXPANDED\":\n",
        "                continue\n",
        "\n",
        "            # Jika semua variabel sudah ditetapkan (solusi lengkap)\n",
        "            if len(fixed_y) == self.n:\n",
        "                # Cek kelayakan penuh untuk solusi ini (seharusnya sudah di-filter oleh pruning)\n",
        "                total_y_ones = sum(fixed_y.values())\n",
        "                total_py_sum = sum(self.p[i-1] * fixed_y[i] for i in fixed_y if fixed_y[i] == 1)\n",
        "\n",
        "                if total_y_ones == self.target_y and total_py_sum >= self.target_budget:\n",
        "                    # Solusi feasible ditemukan, update incumbent jika lebih baik\n",
        "                    if lb < self.best_value:\n",
        "                        self.best_value = lb\n",
        "                        self.best_solution_y = fixed_y.copy()\n",
        "                continue\n",
        "\n",
        "            # Branching: Pilih variabel y berikutnya sesuai self.order\n",
        "            next_y_var_idx = -1\n",
        "            for y_idx in self.order:\n",
        "                if y_idx not in fixed_y:\n",
        "                    next_y_var_idx = y_idx\n",
        "                    break\n",
        "\n",
        "            if next_y_var_idx == -1: # Seharusnya tidak terjadi jika len(fixed_y) != self.n\n",
        "                continue\n",
        "\n",
        "            # Buat 2 cabang: y = 1 dan y = 0\n",
        "            for val in [1, 0]:\n",
        "                new_fixed_y = fixed_y.copy()\n",
        "                new_fixed_y[next_y_var_idx] = val\n",
        "                new_lb = self.lower_bound(new_fixed_y)\n",
        "\n",
        "                self.counter += 1\n",
        "                heapq.heappush(\n",
        "                    pq,\n",
        "                    (new_lb, self.counter, new_fixed_y, current_node_id, f\"y{next_y_var_idx}={val}\")\n",
        "                )\n",
        "\n",
        "        print(\"\\n=== FINAL RESULT (BEST-FIRST) ===\")\n",
        "        print(\"Total nodes:\", self.node_id + 1)\n",
        "        print(\"Minimum (Z'):\", round(self.best_value, 6))\n",
        "        print(\"Optimal y:\", self.best_solution_y)\n",
        "\n",
        "        # Konversi solusi y_optimal kembali ke x_optimal dan hitung Z asli\n",
        "        if self.best_solution_y:\n",
        "            optimal_x = [0] * self.n # Inisialisasi x dengan 0\n",
        "            selected_x_indices_1based = []\n",
        "            total_original_P = 0.0\n",
        "            total_original_p = 0.0\n",
        "\n",
        "            for y_idx_1based, y_val in self.best_solution_y.items():\n",
        "                original_P_idx_0based = y_idx_1based - 1\n",
        "\n",
        "                # x_i = 1 - y_i\n",
        "                x_val = 1 - y_val\n",
        "\n",
        "                # Simpan di array x dengan indeks asli\n",
        "                optimal_x[original_P_idx_0based] = x_val\n",
        "\n",
        "                if x_val == 1:\n",
        "                    selected_x_indices_1based.append(original_P_idx_0based + 1)\n",
        "                    total_original_P += self.P[original_P_idx_0based]\n",
        "                    total_original_p += self.p[original_P_idx_0based]\n",
        "\n",
        "            print(\"Optimal x:\", optimal_x)\n",
        "            print(\"Variabel yang terpilih (Original Index):\", selected_x_indices_1based)\n",
        "            print(\"Maksimum Fungsi Objektif (Z)):\", round(total_original_P, 6))\n",
        "            print(\"Total anggaran:\", round(total_original_p, 1))\n",
        "            print(f\"\\nVerifikasi: ∑x_i = {sum(optimal_x)} (harus {self.k})\")\n",
        "            print(f\"Verifikasi: ∑p_i*x_i = {total_original_p:.1f} (harus ≤ {self.B})\")\n",
        "        else:\n",
        "            print(\"Tidak ditemukan solusi feasible.\")\n",
        "\n",
        "    # =============================\n",
        "    # CETAK NODE (Untuk debugging/visualisasi)\n",
        "    # =============================\n",
        "    def print_nodes(self):\n",
        "        print(\"\"\"\\nNode | Parent | Decision   | Fixed_Y                                 | LB       | Status\\n------------------------------------------------------------------------------------------\"\"\")\n",
        "        for n_data in self.nodes:\n",
        "            fx = \", \".join([f\"y{i}={v}\" for i,v in n_data[\"Fixed_Y\"].items()]) # Reverted to y{i}\n",
        "            print(f\"{n_data['Node']:>4} | {str(n_data['Parent']):>6} | \"\n",
        "                  f\"{n_data['Decision']:<10} | {fx:<40} | \"\n",
        "                  f\"{n_data['LB']:<8} | {n_data['Status']} {n_data['Reason']}\")"
      ],
      "metadata": {
        "id": "8Da4HDJiSBrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = [\n",
        "    0.56011612024394, 0.532157896426633, 0.532157896426633,\n",
        "    0.560130487332721, 0.319457699663552, 0.233660032327584,\n",
        "    0.2408816335929, 0.475695812636056,\n",
        "    0.228420627459742, 0.410232143258209\n",
        "]\n",
        "\n",
        "p = [26, 65, 48.5, 140, 119, 47.7, 99, 60, 69, 37.5]\n",
        "k = 3\n",
        "B = 150\n",
        "\n",
        "model = BalasBestFirst(P, p, k, B)\n",
        "model.solve()\n",
        "model.print_nodes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ8ntmjGVa3m",
        "outputId": "f2fc1815-9b1d-4cb0-fac0-fe9bc970a841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL RESULT (BEST-FIRST) ===\n",
            "Total nodes: 21\n",
            "Minimum Z (Transformed Objective): 2.468478\n",
            "Optimal y: {9: 1, 6: 1, 7: 1, 5: 1, 10: 1, 8: 1, 2: 0, 3: 0, 1: 0, 4: 1}\n",
            "Optimal x (0-based): [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "Selected Items (Original Index, 1-based): [2, 3, 1]\n",
            "Maximum Objective Value (Original P): 1.624432\n",
            "Total Cost (Original p): 139.5\n",
            "\n",
            "Verifikasi: ∑x_i = 3 (harus 3)\n",
            "Verifikasi: ∑p_i*x_i = 139.5 (harus ≤ 150)\n",
            "\n",
            "Node | Parent | Decision   | Fixed_Y                                 | LB       | Status\n",
            "------------------------------------------------------------------------------------------\n",
            "   0 |   None | root       |                                          | 0        | EXPANDED \n",
            "   1 |      0 | y9=1       | y9=1                                     | 2.440506 | EXPANDED \n",
            "   2 |      1 | y6=1       | y9=1, y6=1                               | 2.440506 | EXPANDED \n",
            "   3 |      2 | y7=1       | y9=1, y6=1, y7=1                         | 2.440506 | EXPANDED \n",
            "   4 |      3 | y5=1       | y9=1, y6=1, y7=1, y5=1                   | 2.440506 | EXPANDED \n",
            "   5 |      4 | y10=1      | y9=1, y6=1, y7=1, y5=1, y10=1            | 2.440506 | EXPANDED \n",
            "   6 |      5 | y8=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1      | 2.440506 | EXPANDED \n",
            "   7 |      6 | y2=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=1 | 2.440506 | PRUNED NOT POSSIBLE TO COMPLETE\n",
            "   8 |      6 | y2=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0 | 2.440506 | EXPANDED \n",
            "   9 |      8 | y3=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=1 | 2.440506 | PRUNED NOT POSSIBLE TO COMPLETE\n",
            "  10 |      8 | y3=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=0 | 2.468464 | EXPANDED \n",
            "  11 |     10 | y1=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=0, y1=1 | 2.468464 | PRUNED NOT POSSIBLE TO COMPLETE\n",
            "  12 |     10 | y1=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=0, y1=0 | 2.468478 | EXPANDED \n",
            "  13 |     12 | y4=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=0, y1=0, y4=0 | 1.908348 | PRUNED INFEASIBLE PARTIAL (VIOLATION)\n",
            "  14 |     12 | y4=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=0, y1=0, y4=1 | 2.468478 | EXPANDED \n",
            "  15 |      5 | y8=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=0      | 2.496968 | PRUNED BOUND\n",
            "  16 |      4 | y10=0      | y9=1, y6=1, y7=1, y5=1, y10=0            | 2.562432 | PRUNED BOUND\n",
            "  17 |      3 | y5=0       | y9=1, y6=1, y7=1, y5=0                   | 2.653206 | PRUNED BOUND\n",
            "  18 |      2 | y7=0       | y9=1, y6=1, y7=0                         | 2.731782 | PRUNED BOUND\n",
            "  19 |      1 | y6=0       | y9=1, y6=0                               | 2.739004 | PRUNED BOUND\n",
            "  20 |      0 | y9=0       | y9=0                                     | 2.744243 | PRUNED BOUND\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P = [\n",
        "    0.56011612024394, 0.532157896426633, 0.532157896426633,\n",
        "    0.560130487332721, 0.319457699663552, 0.233660032327584,\n",
        "    0.2408816335929, 0.475695812636056,\n",
        "    0.228420627459742, 0.410232143258209\n",
        "]\n",
        "\n",
        "p = [26, 65, 48.5, 140, 119, 47.7, 99, 60, 69, 37.5]\n",
        "k = 3\n",
        "B = 300\n",
        "\n",
        "model = BalasBestFirst(P, p, k, B)\n",
        "model.solve()\n",
        "model.print_nodes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK5YTeiruBhG",
        "outputId": "dccddec2-0c00-41eb-c017-1eaffe2d4d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL RESULT (BEST-FIRST) ===\n",
            "Total nodes: 27\n",
            "Minimum Z (Transformed Objective): 2.440506\n",
            "Optimal y: {9: 1, 6: 1, 7: 1, 5: 1, 10: 1, 8: 1, 2: 1, 3: 0, 1: 0, 4: 0}\n",
            "Optimal x (0-based): [1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
            "Selected Items (Original Index, 1-based): [3, 1, 4]\n",
            "Maximum Objective Value (Original P): 1.652405\n",
            "Total Cost (Original p): 214.5\n",
            "\n",
            "Verifikasi: ∑x_i = 3 (harus 3)\n",
            "Verifikasi: ∑p_i*x_i = 214.5 (harus ≤ 300)\n",
            "\n",
            "Node | Parent | Decision   | Fixed_Y                                 | LB       | Status\n",
            "------------------------------------------------------------------------------------------\n",
            "   0 |   None | root       |                                          | 0        | EXPANDED \n",
            "   1 |      0 | y9=1       | y9=1                                     | 2.440506 | EXPANDED \n",
            "   2 |      1 | y6=1       | y9=1, y6=1                               | 2.440506 | EXPANDED \n",
            "   3 |      2 | y7=1       | y9=1, y6=1, y7=1                         | 2.440506 | EXPANDED \n",
            "   4 |      3 | y5=1       | y9=1, y6=1, y7=1, y5=1                   | 2.440506 | EXPANDED \n",
            "   5 |      4 | y10=1      | y9=1, y6=1, y7=1, y5=1, y10=1            | 2.440506 | EXPANDED \n",
            "   6 |      5 | y8=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1      | 2.440506 | EXPANDED \n",
            "   7 |      6 | y2=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=1 | 2.440506 | EXPANDED \n",
            "   8 |      6 | y2=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0 | 2.440506 | EXPANDED \n",
            "   9 |      7 | y3=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=1, y3=0 | 2.440506 | EXPANDED \n",
            "  10 |      8 | y3=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=1 | 2.440506 | EXPANDED \n",
            "  11 |      9 | y1=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=1, y3=0, y1=0 | 2.440506 | EXPANDED \n",
            "  12 |     10 | y1=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=1, y1=0 | 2.440506 | EXPANDED \n",
            "  13 |     11 | y4=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=1, y3=0, y1=0, y4=0 | 2.440506 | EXPANDED \n",
            "  14 |     12 | y4=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=1, y1=0, y4=0 | 2.440506 | PRUNED BOUND\n",
            "  15 |      8 | y3=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=0 | 2.468464 | PRUNED BOUND\n",
            "  16 |      5 | y8=0       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=0      | 2.496968 | PRUNED BOUND\n",
            "  17 |      4 | y10=0      | y9=1, y6=1, y7=1, y5=1, y10=0            | 2.562432 | PRUNED BOUND\n",
            "  18 |      3 | y5=0       | y9=1, y6=1, y7=1, y5=0                   | 2.653206 | PRUNED BOUND\n",
            "  19 |      2 | y7=0       | y9=1, y6=1, y7=0                         | 2.731782 | PRUNED BOUND\n",
            "  20 |      1 | y6=0       | y9=1, y6=0                               | 2.739004 | PRUNED BOUND\n",
            "  21 |      0 | y9=0       | y9=0                                     | 2.744243 | PRUNED BOUND\n",
            "  22 |      7 | y3=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=1, y3=1 | 2.972664 | PRUNED BOUND\n",
            "  23 |      9 | y1=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=1, y3=0, y1=1 | 3.000622 | PRUNED BOUND\n",
            "  24 |     10 | y1=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=1, y1=1 | 3.000622 | PRUNED BOUND\n",
            "  25 |     11 | y4=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=1, y3=0, y1=0, y4=1 | 3.000636 | PRUNED BOUND\n",
            "  26 |     12 | y4=1       | y9=1, y6=1, y7=1, y5=1, y10=1, y8=1, y2=0, y3=1, y1=0, y4=1 | 3.000636 | PRUNED BOUND\n"
          ]
        }
      ]
    }
  ]
}